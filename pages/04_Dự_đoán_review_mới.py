import streamlit as st
import pandas as pd
import numpy as np
import pickle
from underthesea import word_tokenize, pos_tag, sent_tokenize
import regex


st.set_page_config(page_title="MÃ´ táº£ dá»± Ã¡n",
                   layout="wide",
                   initial_sidebar_state="auto")


st.markdown("# <center>Project 1:<span style='color:#4472C4; font-family:Calibri (Body);font-style: italic;'> Sentiment Analysis</span></center>", unsafe_allow_html=True)

st.subheader("Nháº­p vÃ o comment", divider='rainbow')

# Cho ngÆ°á»i dÃ¹ng chá»n nháº­p dá»¯ liá»‡u hoáº·c upload file
type = st.radio("Chá»n cÃ¡ch nháº­p dá»¯ liá»‡u", options=["Nháº­p dá»¯ liá»‡u trá»±c tiáº¿p", "Upload file"])
lst = []
if type == "Nháº­p dá»¯ liá»‡u trá»±c tiáº¿p":
    comment1 = st.text_input("Text 1")
    if comment1:
        lst.append(comment1)
    comment2 = st.text_input("Text 2")
    if comment2:
        lst.append(comment2)
elif type == "Upload file":
    st.subheader("Upload file")
    # Upload file
    uploaded_file = st.file_uploader("Chá»n file dá»¯ liá»‡u", type=["xlsx"])
    if uploaded_file is not None:
        # Äá»c file dá»¯ liá»‡u
        df = pd.read_excel(uploaded_file)
        print(df.columns.tolist())
        if len(df.columns) == 1 and df.columns.tolist()[0] == "New_Comment":
            lst = df["New_Comment"].tolist()
        else:
            st.warning('file upload chá»©a dá»¯ liá»‡u khÃ´ng há»£p lá»‡!', icon="âš ï¸")
        
        # st.write(df)
    with st.expander("**YÃªu cáº§u file upload nhÆ° sau:**"):
        st.markdown("""
* File cÃ³ Ä‘á»‹nh dáº¡ng **.xlsx**
* CÃ³ 1 cá»™t: **New_Comment**
* VÃ­ dá»¥:

| New_Comment |
|---|
| MÃ³n nÆ°á»›ng ngon quÃ¡!  |
| Äá»“ Äƒn hÃ´m nay dá»Ÿ thiá»‡t!  |
                """)
        st.markdown('<div style="padding: 10px 5px;"></div>', unsafe_allow_html=True)
        st.markdown("* Hoáº·c download **file máº«u** nhÆ° sau:")
        df_sample = pd.DataFrame({"New_Comment": ["MÃ³n nÆ°á»›ng ngon quÃ¡!",
                                                  "Äá»“ Äƒn hÃ´m nay dá»Ÿ thiá»‡t!",
                                                  "Sá»£ á»‘c cÃ¡t nhÆ°ng k há» nhÃ©. Ráº¥t ok so vá»›i giÃ¡ luÃ´n. Cháº¥t lÆ°á»£ng ok. Äá»“ Äƒn tÆ°Æ¡i, tÃ´m sá»‘ng. CÃ³ gháº¹ ná»¯a.",
                                                  "GiÃ¡ ráº». Äá»“ Äƒn ngon, Æ°á»›p tháº¥m vá»‹. QuÃ¡n sáº¡ch sáº½, khÃ´ng gian  thoÃ¡ng. NhÃ¢n viÃªn ráº¥t dá»… thÆ°Æ¡ng, chu Ä‘Ã¡o.",
                                                  "QuÃ¡n rá»™ng rÃ£i, Ä‘á»“ Äƒn Ä‘Æ°á»£c, giÃ¡ há»£p lÃ½, lÃªn mÃ³n nhiá»u Äƒn thoáº£i mÃ¡i. KhÃ´ng bá»‹ phá»¥ thu, tiá»n nÆ°á»›c khÃ´ng quÃ¡ cao",
                                                  "Báº£o vá»‡ quÃ¡n nÃ y khÃ´ng dáº¯t xe khi trá»i mÆ°a.TrÃ­ch nguyÃªn vÄƒn lá»i anh bv: â€œbáº£o vá»‡ chá»‰ cÃ³ ngá»“i coi xe chá»© khÃ´ng cÃ³ dáº¯t xeâ€"]})
        # st.dataframe(df_sample, hide_index=True)
        df_sample.to_excel('sample_comment.xlsx', index=False)
        # Táº¡o download button
        with open('sample_comment.xlsx', 'rb') as f:
            file_contents = f.read()
        st.download_button(
            label="Download sample excel file",
            data=file_contents,
            file_name='sample_comment.xlsx',
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )


# Chuyá»ƒn comment vÃ o dataframe
new_df = pd.DataFrame({"New_Comment": lst})
st.dataframe(new_df, width=1200, height=200)

## Xá»­ lÃ½ tiáº¿ng Viá»‡t
##LOAD EMOJICON
file = open('files/emojicon.txt', 'r', encoding="utf8")
emoji_lst = file.read().split('\n')
emoji_dict = {}
for line in emoji_lst:
    key, value = line.split('\t')
    emoji_dict[key] = str(value)
#print(teen_dict)
file.close()
#################
#LOAD TEENCODE
file = open('files/teencode.txt', 'r', encoding="utf8")
teen_lst = file.read().split('\n')
teen_dict = {}
for line in teen_lst:
    key, value = line.split('\t')
    teen_dict[key] = str(value)
#print(teen_dict)
file.close()
###############
#LOAD TRANSLATE ENGLISH -> VNMESE
file = open('files/english-vnmese.txt', 'r', encoding="utf8")
englist_lst = file.read().split('\n')
for line in englist_lst:
    key, value = line.split('\t')
    teen_dict[key] = str(value)
#print(teen_dict)
file.close()
################
#LOAD wrong words. Bá» 1 sá»‘ tá»« liÃªn quan Ä‘áº¿n "ngon".
file = open('files/wrong-word2.txt', 'r', encoding="utf8")
wrong_lst = file.read().split('\n')
file.close()
#################
#LOAD STOPWORDS
file = open('files/vietnamese-stopwords.txt', 'r', encoding="utf8")
stopwords_lst = file.read().split('\n')
file.close()

# Táº¡o hÃ m xá»­ lÃ½ emoji, teencode, translate, wrong words, stopwords
def process_text(text, dict_emoji, dict_teen, lst_wrong):
    document = text.lower()
    document = document.replace("â€™",'')
    document = regex.sub(r'\.+', ".", document)
    new_sentence =''
    for sentence in sent_tokenize(document):
        # if not(sentence.isascii()):
        ###### CONVERT EMOJICON
        sentence = ''.join(dict_emoji[word]+' ' if word in dict_emoji else word for word in list(sentence))
        ###### CONVERT TEENCODE
        sentence = ' '.join(dict_teen[word] if word in dict_teen else word for word in sentence.split())
        ###### DEL Punctuation & Numbers
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(regex.findall(pattern,sentence))
        ###### DEL wrong words
        sentence = ' '.join('' if word in lst_wrong else word for word in sentence.split())
        new_sentence = new_sentence+ sentence + '. '
    document = new_sentence
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    return document

def loaddicchar():
    uniChars = "Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"
    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
        '|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic

# ÄÆ°a toÃ n bá»™ dá»¯ liá»‡u qua hÃ m nÃ y Ä‘á»ƒ chuáº©n hÃ³a láº¡i
def covert_unicode(txt):
    dicchar = loaddicchar()
    return regex.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dicchar[x.group()], txt)

import re
# HÃ m Ä‘á»ƒ chuáº©n hÃ³a cÃ¡c tá»« cÃ³ kÃ½ tá»± láº·p
def normalize_repeated_characters(text):
    # Thay tháº¿ má»i kÃ½ tá»± láº·p liÃªn tiáº¿p báº±ng má»™t kÃ½ tá»± Ä‘Ã³
    # VÃ­ dá»¥: "ngonnnn" thÃ nh "ngon", "thiá»‡tttt" thÃ nh "thiá»‡t"
    return re.sub(r'(.)\1+', r'\1', text)

def remove_stopword(text, stopwords):
    ###### REMOVE stop words
    document = ' '.join('' if word in stopwords else word for word in text.split())
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    return document

## Tiá»n xá»­ lÃ½ Tiáº¿ng Viá»‡t cho comment má»›i
# Xá»­ lÃ½ emoji, teencode, wrong word
if new_df.size != 0:
    new_df = new_df.assign(Comment_2=lambda x: x['New_Comment'].apply(lambda x: process_text(str(x), emoji_dict, teen_dict, wrong_lst)))
    # Chuáº©n hÃ³a unicode
    new_df = new_df.assign(Comment_2=lambda x: x['Comment_2'].apply(lambda x:  covert_unicode(str(x))))
    # Chuáº©n hÃ³a kÃ½ tá»± láº·p
    new_df = new_df.assign(Comment_2=lambda x: x['Comment_2'].apply(lambda x:  normalize_repeated_characters(str(x))))
    # Loáº¡i bá» stopword
    new_df = new_df.assign(Comment_2=lambda x: x['Comment_2'].apply(lambda x:  remove_stopword(str(x), stopwords_lst)))

def predict_comment(df):
    ## Load model TfIDF Ä‘Ã£ save
    # ÄÆ°á»ng dáº«n file model Tfidf
    path_tfidf = "data/tfidf02_model.pkl"
    # Load model
    with open(path_tfidf, 'rb') as f:
        tfidf_model = pickle.load(f)
    ## Transform data comment má»›i
    # Táº¡o dataframe má»›i chá»©a cÃ¡c tfidf features
    df_tfidf02 = pd.DataFrame(tfidf_model.transform(df['Comment_2']).toarray(), columns=tfidf_model.get_feature_names_out())
        
    ## Load model Logistic Regression
    # ÄÆ°á»ng dáº«n file model Logistic Regression  
    path_lr = "data/lr_smote_model.pkl"
    # Load model
    with open(path_lr, 'rb') as f:
        lr_model = pickle.load(f)
        
    ## Dá»± Ä‘oÃ¡n comment má»›i
    # Dá»± Ä‘oÃ¡n
    pred = lr_model.predict(df_tfidf02)
    # Chuyá»ƒn káº¿t quáº£ vÃ o dataframe
    new_df["pred"] = pred

    new_df["ÄÃ¡nh giÃ¡"] = new_df["pred"].map({0: "ğŸ˜¡", 1: "ğŸ‘"})
    ## Load dataframe sau khi xá»­ lÃ½
    st.subheader("Káº¿t quáº£ dá»± Ä‘oÃ¡n", divider='rainbow')
    st.dataframe(new_df[["New_Comment", "ÄÃ¡nh giÃ¡"]],
                column_config={
                    "New_Comment": "Báº¡n Ä‘Ã£ nháº­p",
                    "ÄÃ¡nh giÃ¡": {'alignment': 'center'}
                    },
                hide_index=True, width=1200, height=200)
    st.write("*Ghi chÃº*")
    st.write('ğŸ‘ = TÃ­ch cá»±c, ğŸ˜¡ = TiÃªu cá»±c')
    
if new_df.size != 0:
    predict_comment(new_df)

